{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "### *The 12th Computational Neuroscience Winter School*\n",
    "\n",
    "# Tutorial I: Brain Inspired Computation - Advanced Part\n",
    "----\n",
    "__Date:__ Jan. 10, 2023\n",
    "\n",
    "__Content Creator:__ Chongming Liu"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial Objectives\n",
    "\n",
    "In this notebook, we'll learn the development of brain inspired computer vision, mainly about the convolutional neural network (CNN) and its variational forms. And at the last part of this section, we would also have a discussion on the future direction in terms of brain inspired computer vision.\n",
    "\n",
    "1. Convolution and LeNet-5\n",
    "1. AlexNet and visual system"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Setup\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import most modules and functions needed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['lines.linewidth'] = 2.5\n",
    "mpl.rcParams['legend.fontsize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "mpl.rcParams['axes.titlesize'] = 14\n",
    "mpl.rcParams['font.weight'] = 'bold'\n",
    "mpl.rcParams['font.size'] = 12\n",
    "mpl.rcParams['axes.labelweight'] = 'bold'\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def setup_seed(seed):\n",
    "    '''\n",
    "    setting random seed\n",
    "    '''\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    return\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Section 1: Convolution and LeNet-5\n",
    "\n",
    "*Reference:* \n",
    "\n",
    "*Hubel, D.H., & Wiesel, T.N. (1962). Receptive fields, binocular interaction and functional architecture in the cat's visual cortex. The Journal of Physiology, 160.*\n",
    "\n",
    "*Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36, 193-202.*\n",
    "\n",
    "*LeCun, Y., Jackel, L.D., Bottou, L., Cortes, C., Denker, J.S., Drucker, H., Subramanian, I., Muller, U., Sackinger, E., Simard, P.Y., & Vapnik, V.N. (1995). Learning algorithms for classification: A comparison on handwritten digit recognition.*\n",
    "\n",
    "*LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proc. IEEE, 86, 2278-2324.*\n",
    "\n",
    "*LeCun, Y., & Cortes, C. (2005). The mnist database of handwritten digits.*\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "David H. Hubel and Torsten Wiesel first discovered there existed receptive fields, binocular interaction and functional architecture in the cat's visual cortex in 1962. Inspired by the receptive field, Fukushima brought out the operation \"convolution\" in the neocognitron in 1980. \n",
    "\n",
    "<img src=\"./imgs/Conv.gif\" width=\"50%\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then LeCun et al. combine the convolution with neural network and also with gradient-based learning algorithm and proposed the first convolutional neural network (CNN): LeNet. \n",
    "\n",
    "<img src=\"./imgs/LeNet.png\" width=\"80%\">\n",
    "\n",
    "Important hyperparameters: number of channels, kernel size, padding, stride. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# LeNet-5 for training MNIST dataset  #Original resolution: 1*28*28\n",
    "setup_seed(1)\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x)) # Size after this layer: ...\n",
    "        out = F.max_pool2d(out, 2) # Size after this layer: ...\n",
    "        out = F.relu(self.conv2(out)) # Size after this layer: ...\n",
    "        out = F.max_pool2d(out, 2) # Size after this layer: ...\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "train_dataset = torchvision.datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "valid_dataset = torchvision.datasets.MNIST(root='data', train=False, transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=10000)\n",
    "\n",
    "model = LeNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "test_accuracy = []\n",
    "best_prec = 0\n",
    "\n",
    "print(model)\n",
    "for j, (input, target) in enumerate(valid_loader):\n",
    "    input, target = input, target.long()\n",
    "    \n",
    "fig,ax=plt.subplots(1,6, layout=\"constrained\", figsize=(10,18))\n",
    "for i in range(6):\n",
    "    ax[i].imshow(input[i,:,:,:].reshape(28,28), cmap = plt.cm.gray)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Training process\n",
    "for epoch in range(0, 1):\n",
    "    \n",
    "    model.train()\n",
    "    # train for one epoch\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        train_total = 0\n",
    "        train_correct = 0\n",
    "        train_loss = 0\n",
    "        # measure data loading time\n",
    "        input, target = input, target.long()\n",
    "                    \n",
    "        \n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        train_total += target.size(0)\n",
    "        train_correct += (predicted == target).sum().item()\n",
    "        prec = train_correct / train_total\n",
    "        \n",
    "        if (i) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.5f}, Train_Acc:{:.2f}%'.format(epoch+1, 10, i, len(train_loader), loss, prec*100))\n",
    "\n",
    "            \n",
    "            model.eval()\n",
    "            valid_correct = 0\n",
    "            valid_total = 0\n",
    "            with torch.no_grad():\n",
    "                total_loss = 0\n",
    "                for j, (input, target) in enumerate(valid_loader):\n",
    "                    input, target = input, target.long()\n",
    "                    output = model(input)\n",
    "                    \n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    valid_total = output.shape[0]\n",
    "                    valid_correct = (predicted == target).sum().item()\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "        \n",
    "                    prec = valid_correct / valid_total\n",
    "                    print('Accuary on test images:{:.2f}%'.format(prec*100))\n",
    "                    test_accuracy.append(prec)\n",
    "                    best_prec = max(prec, best_prec)\n",
    "                \n",
    "\n",
    "print('Best accuracy is: {:.2f}%'.format(best_prec*100))\n",
    "plt.figure(figsize=(3.5,2))\n",
    "plt.plot(100*torch.arange(len(test_accuracy)), test_accuracy,linewidth=2)\n",
    "plt.xlabel('# training steps')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Section 2: AlexNet and visual system\n",
    "\n",
    "*Reference:* \n",
    "\n",
    "*Krizhevsky, A., Sutskever, I., & Hinton, G.E. (2012). ImageNet classification with deep convolutional neural networks. Communications of the ACM, 60, 84 - 90.*\n",
    "\n",
    "*Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res., 15, 1929-1958.*\n",
    "\n",
    "Zeiler, M.D., & Fergus, R. (2013). Visualizing and Understanding Convolutional Networks. European Conference on Computer Vision.\n",
    "\n",
    "*Schrimpf, M., Kubilius, J., Hong, H., Majaj, N.J., Rajalingham, R., Issa, E.B., Kar, K., Bashivan, P., Prescott-Roy, J., Schmidt, K., Yamins, D., & DiCarlo, J.J. (2018). Brain-Score: Which Artificial Neural Network for Object Recognition is most Brain-Like? bioRxiv.*\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ImageNet Large-Scale Visual Recognition Challenge(ILSVRC) is the 'world cup' in computer vision. \n",
    "\n",
    "<img src=\"./imgs/ILSVRC.jpeg\" width=\"50%\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In 2012, Hinton and his student Alex Krizhevsky brought out AlexNet and won the champion of ILSVRC at that year. Spotlight: ReLU activate function and dropout layer.\n",
    "\n",
    "<img src=\"./imgs/alexnet.png\" width=\"80%\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torchvision.io import read_image\n",
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "\n",
    "# Initialize model with the best available weights\n",
    "weights = AlexNet_Weights.DEFAULT\n",
    "model = alexnet(weights=weights)\n",
    "\n",
    "# Visualize the convolution kernel in the first layer\n",
    "feature_image = model.state_dict()['features.0.weight'].clone()\n",
    "\n",
    "fig,ax=plt.subplots(4,16, figsize=(10,2.5),constrained_layout=True)\n",
    "for i in range(64):\n",
    "    axi_x = int(i/16)\n",
    "    axi_y = i-axi_x*16\n",
    "    feature_image = model.state_dict()['features.0.weight'][i,:,:,:]\n",
    "    feature_image-= feature_image.mean()\n",
    "    feature_image/= feature_image.std ()\n",
    "    feature_image*=  64\n",
    "    feature_image+= 128\n",
    "    kernel_show = np.clip(feature_image.numpy(),0,255).astype('uint8')\n",
    "    ax[axi_x,axi_y].imshow(np.transpose((kernel_show),(1,2,0)))\n",
    "    ax[axi_x,axi_y].set_xticks([])\n",
    "    ax[axi_x,axi_y].set_yticks([])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Using AlexNet to do image classification\n",
    "img = Image.open('./ImageNet_examples/shark.jpeg')\n",
    "img"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Using AlexNet to do image classification\n",
    "weights = AlexNet_Weights.DEFAULT\n",
    "model = alexnet(weights=weights)\n",
    "model.eval()\n",
    "# Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Apply inference preprocessing transforms\n",
    "img = preprocess(img).unsqueeze(0)\n",
    "\n",
    "# Step 4: Use the model and print the predicted category\n",
    "prediction = model(img).squeeze(0).softmax(0)\n",
    "_,class_id = torch.sort(prediction,descending = True)\n",
    "for i in range(5):\n",
    "    category_name = weights.meta[\"categories\"][class_id[i]]\n",
    "    score = prediction[class_id[i]]\n",
    "    print(f\"{category_name}: {100 * score:.1f}%\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To know how the network recognize the target, Zeiler et al. (2013) proposed a method that we can visualize the gradient of a specific input with respect to the output at the position of corresponding label. Like given data(image) $(x_n, y_n)$, and let $o(x)$ stands for the output vector for an input data $x$. Then the gradient is:\n",
    "\n",
    "$$grad(x_n) = \\frac{\\partial o_{y_n}(x_n)}{\\partial x}$$\n",
    "\n",
    "This gradient means the network focus on which part of this picture to do the classification.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# How the network recognize the target?\n",
    "def gradient(img, target_class):\n",
    "    '''\n",
    "    Return the gradient matrix of \n",
    "    '''\n",
    "    img.requires_grad = True\n",
    "    # Computer the output and gradient\n",
    "    output = model(img).reshape(1000)[...]\n",
    "    output.backward()\n",
    "    return np.transpose((img.grad.data.squeeze(0).numpy()),(1,2,0))\n",
    "\n",
    "\n",
    "def transform(rgb_matrix):\n",
    "    '''\n",
    "    Return a linear transform of the origin matrix that all elements belong to [0,1].\n",
    "    '''\n",
    "    min_matrix = np.min(rgb_matrix)\n",
    "    matrix = rgb_matrix - min_matrix\n",
    "    matrix = matrix/np.max(matrix)\n",
    "    return matrix\n",
    "\n",
    "target_class = class_id[0]\n",
    "\n",
    "fig,ax=plt.subplots(1,3, figsize=(10,3.5),constrained_layout=True)\n",
    "ax[0].imshow(transform(np.transpose((img.squeeze(0).numpy()),(1,2,0))))\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "\n",
    "gradient = transform(gradient(img,target_class))\n",
    "#denosing\n",
    "#gradient[gradient<np.max(gradient)/2.5] = 0\n",
    "\n",
    "ax[1].imshow(gradient)\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "\n",
    "img = Image.open('./ImageNet_examples/shark.jpeg')\n",
    "img = preprocess(img).unsqueeze(0)\n",
    "ax[2].imshow(transform(gradient*10+transform(np.transpose((img.squeeze(0).numpy()),(1,2,0)))))\n",
    "ax[2].set_xticks([])\n",
    "ax[2].set_yticks([])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The way of how CNN doing object recognition are very similar to how visual system is doing.\n",
    "\n",
    "<img src=\"./imgs/brainscore.png\" width=\"80%\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "- In this tutorial, we learned the basic operation of convolution and then construct two different convolutional neural networks (CNN)- LeNet-5 and AlexNet, and then we use them to do MNIST and CIFAR10 classification, respectively.\n",
    "- LeNet-5 is the first convolutional neural network that was widely used in many object recognition tasks, it is the basic of the modern framework of CNNs.\n",
    "- AlexNet is the first 'deep' convolutional neural network, and won the ILSVRC in 2012. It is a brain-inspired net in the sense of using dropout layer, ReLU activation function and so on. Moreover, we find that it can natually perform like the visual system in the brain. And also with the brain-score we can conclude that if we want to reach high-performance like human, we must consider more about the properties in the visual system.\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}